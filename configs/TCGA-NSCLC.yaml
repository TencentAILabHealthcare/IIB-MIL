# CONFIG


DATA_TABLE: ""
NUM_FOLD: 5
LABEL_NAME: "Label"


PATCH:
    IDX2IMG: '/aaa/louisyuzhao/guy1/qinren/DataSet/hyy_first_processed/ dict_name2imgs_path_rmbg2.pkl'

PRETRAIN:
    MODEL_NAME: 'efficientnet-b0'
    MODEL_PATH: './checkpoints/Efficientnet'
    SAVE_DIR: './resultpretrained_features_rmbg_ori'
    SAVE_DIR_COMBINED: './resultpretrained_features_combined_rmbg_ori'
    TRAIN_FEA_TIMES: 1
#model: MODEL_NAME, NUM_INPUT_CHANNELS, NUM_CLASSES

MODEL:
    MODEL_NAME: 'iibmil' # transformer, amil, cnn, vit, pure_transformer
    NUM_CLASSES: 2

#dataset: DATASET_NAME, DATASET_SEED

DATASET:
    DATASET_NAME: 'patch_confidence'
    DATASET_SEED: 31
    DATASET_SCALE: 'x20'
    FEATURE_MAP_SIZE: 80
    PATCH_LEVEL: True
    FEATURE_LEN: 500

MODEL_IIBMIL:
    NUM_CLASSES: 2
    DROPOUT: 0.0
    NUM_INPUT_CHANNELS: 1280 # feature dim
    HIDDEN_DIM: 256 # Size of the embeddings (dimension of the transformer)
    DEPTH: 3
    HEADS: 4
    MLP_DIM: 32

# train: EPOCHS, BATCH_SIZE, LR
TRAIN:
    LOSS_NAME: 'focal'
    EPOCHS: 300 # 200
    START_EPOCH: 0
    BATCH_SIZE: 16
    NUM_WORKERS: 10
    LR: 1e-5
    LR_DROP: 100
    LR_BACKBONE: 2e-5
    LR_LINEAR_PROJ_MULT: 0.1
    WEIGHT_DECAY: 1e-5 #5e-2
    CLIP_MAX_NORM: 0.01
    OPTIM_NAME: 'adamw'
    EVAL: False
    RESUME_PATH: './checkpoints/IIB-MIL/TCGA-NSCLC.pth'
    OUTPUT_DIR: './result'
    SEED: 666
    CACHE_MODE: False
    CLS_LOSS_COEF: 2 # for transformer below
    BOX_LOSS_COEF: 5
    GIOU_LOSS_COEF: 2
    MASK_LOSS_COEF: 1
    DICE_LOSS_COEF: 1
    FOCAL_ALPHA: 0.25
    EMA: 0.99    ####0.85
    WARMUP: 10   ####5