# CONFIG

TASK: "" # Task Name
DATA_TABLE: ""
LABEL_NAME: "Label"

PATCH:
    IDX2IMG: "./result/tiled_patches/dict_name2imgs.pkl" # Path indicating the place to save the tiled patches.

PRETRAIN:
    MODEL_NAME: 'efficientnet-b0' # patch feature extraction model name
    MODEL_PATH: './checkpoints/Efficientnet/efficientnet-b0-355c32eb.pth' # Path storing the pre-trained model document
    SAVE_DIR: './result/pretrained_features' # Path indicating the place to save the extracted patch features
    SAVE_DIR_COMBINED: './result/pretrained_features_combined' # Path indicating the place to save the combinded extracted patch features (all path features in a WSI is stored as one data file.)
    TRAIN_FEA_TIMES: 1
#model: MODEL_NAME, NUM_INPUT_CHANNELS, NUM_CLASSES

MODEL:
    MODEL_NAME: 'iibmil' # transformer, amil, cnn, vit, pure_transformer
    NUM_CLASSES: 3

#dataset: DATASET_NAME, DATASET_SEED

DATASET:
    DATASET_NAME: 'patch_confidence'
    DATASET_SEED: 5
    DATASET_SCALE: 'x20'
    FEATURE_MAP_SIZE: 80
    PATCH_LEVEL: True
    FEATURE_LEN: 500

MODEL_IIBMIL:
    NUM_CLASSES: 3
    DROPOUT: 0.0
    NUM_INPUT_CHANNELS: 1280 # feature dim
    HIDDEN_DIM: 256 # Size of the embeddings (dimension of the transformer)
    DEPTH: 4
    HEADS: 4
    MLP_DIM: 32

# train: EPOCHS, BATCH_SIZE, LR
TRAIN:
    LOSS_NAME: 'focal'
    EPOCHS: 300 # 200
    START_EPOCH: 0
    BATCH_SIZE: 4
    NUM_WORKERS: 10
    LR: 1e-5
    LR_DROP: 100
    LR_BACKBONE: 2e-5
    LR_LINEAR_PROJ_MULT: 0.1
    WEIGHT_DECAY: 1e-5 #5e-2
    CLIP_MAX_NORM: 0.01
    OPTIM_NAME: 'adamw'
    EVAL: False
    RESUME_PATH: './checkpoints/IIB-MIL/TCGA-RCC.pth'
    OUTPUT_DIR: './result'
    SEED: 666
    CACHE_MODE: False
    CLS_LOSS_COEF: 2 # for transformer below
    BOX_LOSS_COEF: 5
    GIOU_LOSS_COEF: 2
    MASK_LOSS_COEF: 1
    DICE_LOSS_COEF: 1
    FOCAL_ALPHA: 0.25
    EMA: 0.99    ####0.85
    WARMUP: 10   ####5